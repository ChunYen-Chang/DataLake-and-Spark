{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Data lake for Sparkify (By using AWS S3)\n",
    "- #### Project object:\n",
    "Build an ETL pipeline that extracts Sparkify user activities data (song_data and log_data) which resides in Sparkify data lake (AWS S3), processes the data using Spark cluster, and loads the data back into AWS S3 as a set of dimensional tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PART 1. READ DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Doub, StringType as Str, LongType as Long, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Read the AWS access key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS CREDS', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS CREDS', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Define the input and output file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://fikruanktest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Load song_data and log_data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for song_data\n",
    "SongSchema = R([\n",
    "                Fld(\"artist_id\",Str()),\n",
    "                Fld(\"artist_latitude\",Doub()),\n",
    "                Fld(\"artist_location\",Str()),\n",
    "                Fld(\"artist_longitude\",Doub()),\n",
    "                Fld(\"artist_name\",Str()),\n",
    "                Fld(\"duration\",Doub()),\n",
    "                Fld(\"num_songs\",Long()),\n",
    "                Fld(\"song_id\",Str()),\n",
    "                Fld(\"title\",Str()),\n",
    "                Fld(\"year\",Long()),\n",
    "               ])\n",
    "\n",
    "# load song_data from S3 bucket\n",
    "song_data = os.path.join(input_data, \"song_data/A/*/*/*.json\")\n",
    "song_df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for log_data\n",
    "LogSchema = R([\n",
    "                Fld(\"artist\",Str()),\n",
    "                Fld(\"auth\",Str()),\n",
    "                Fld(\"firstName\",Str()),\n",
    "                Fld(\"gender\",Str()),\n",
    "                Fld(\"itemInSession\",Long()),\n",
    "                Fld(\"lastName\",Str()),\n",
    "                Fld(\"length\",Doub()),\n",
    "                Fld(\"level\",Str()),\n",
    "                Fld(\"location\",Str()),\n",
    "                Fld(\"method\",Str()),\n",
    "                Fld(\"page\",Str()),\n",
    "                Fld(\"registration\",Doub()),\n",
    "                Fld(\"sessionId\",Long()),\n",
    "                Fld(\"song\",Str()),\n",
    "                Fld(\"status\",Long()),\n",
    "                Fld(\"ts\",Long()),\n",
    "                Fld(\"userAgent\",Str()),\n",
    "                Fld(\"userId\",Str()),\n",
    "              ])\n",
    "\n",
    "# load log_data from S3 bucket\n",
    "log_data = os.path.join(input_data, \"log_data/*/*/*.json\")\n",
    "log_df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PART 2. DEAL WITH SONG_DATA AND LOG_DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Deal with song_data(extract data to form artist, song tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARTIST TABLE PART\n",
    "# extract columns from song_df, drop duplicated value, drop none value\n",
    "artist_df = song_df.select(['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude'])\n",
    "artist_df = artist_df.dropDuplicates(['artist_id'])\n",
    "artist_df = artist_df.dropna(how = \"any\", subset = [\"artist_id\"])\n",
    "artist_df = artist_df.filter(artist_df.artist_id != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "artist_df.sort('artist_id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save artist table to s3 in parquet format\n",
    "artist_df.write.parquet(\"{}/artist_table.parquet\".format(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SONG TABLE PART\n",
    "# extract columns from song_df, drop duplicated value, drop none value\n",
    "songinf_df = song_df.select(['song_id', 'title', 'artist_id', 'year', 'duration'])\n",
    "songinf_df = songinf_df.dropDuplicates(['song_id'])\n",
    "songinf_df = songinf_df.dropna(how = \"any\", subset = [\"song_id\"])\n",
    "songinf_df = songinf_df.filter(songinf_df.song_id != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "songinf_df.sort('song_id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save song table to s3 in parquet format\n",
    "songinf_df.write.partitionBy(\"year\", \"artist_id\").parquet(\"{}/song_table.parquet\".format(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Deal with log_data(extract data to form user, time and songplay tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER TABLE PART\n",
    "# extract columns from song_df, drop duplicated value, drop none value\n",
    "user_df = log_df.select(['userId', 'firstName', 'lastName', 'gender', 'level'])\n",
    "user_df = user_df.dropDuplicates(['userId'])\n",
    "user_df = user_df.dropna(how = \"any\", subset = [\"userId\"])\n",
    "user_df = user_df.filter(user_df.userId != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "user_df.sort('userId').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save user table to s3 in parquet format\n",
    "user_df.write.parquet(\"{}/user_table.parquet\".format(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME TABLE PART\n",
    "# define a function for convert ts column in log_df\n",
    "def convert_timestamp(x):\n",
    "    datetime_data = datetime.fromtimestamp(x/1000)\n",
    "    return datetime_data\n",
    "\n",
    "# register this function by udf function so that spark can use it\n",
    "convert_timestamp_udf=udf(convert_timestamp, TimestampType())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the value in ts column into datetime format, drop duplicated value, drop none value\n",
    "time_df = log_df.select(['ts'])\n",
    "time_df = time_df.dropDuplicates(['ts'])\n",
    "time_df = time_df.dropna(how = \"any\", subset = [\"ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data in time table\n",
    "time_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract start_time, hour, day, week, month, year, weekday from ts column\n",
    "time_df = time_df.withColumn('start_time',convert_timestamp_udf('ts'))\n",
    "time_df = time_df.withColumn('hour', F.hour('start_time'))\n",
    "time_df = time_df.withColumn('day', F.dayofmonth('start_time'))\n",
    "time_df = time_df.withColumn('week', F.weekofyear('start_time'))\n",
    "time_df = time_df.withColumn('month', F.month('start_time'))\n",
    "time_df = time_df.withColumn('year', F.year('start_time'))\n",
    "time_df = time_df.withColumn('weekday', F.dayofweek('start_time'))\n",
    "\n",
    "# form the final time table\n",
    "time_df = time_df.select(['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data in time table\n",
    "time_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save time table to s3 in parquet format\n",
    "time_df.write.partitionBy(\"year\", \"month\").parquet(\"{}/time_table.parquet\".format(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SONGPLAY TABLE PART\n",
    "# filter the log_df (extract \"the value in page is equal to NextSong\")\n",
    "log_df_filter = log_df.where(log_df.page == 'NextSong')\n",
    "\n",
    "# create start_time column for log_df_filter\n",
    "log_df_filter = log_df_filter.withColumn('start_time',convert_timestamp_udf('ts'))\n",
    "\n",
    "# define the cond variable for the following join operation\n",
    "cond = [log_df_filter.artist == song_df.artist_name, \n",
    "        log_df_filter.song == song_df.title,\n",
    "        log_df_filter.length == song_df.duration]\n",
    "\n",
    "# join log_df_filter and song_df\n",
    "songplay_df = log_df_filter.join(song_df, cond) \\\n",
    "                            .select([F.monotonically_increasing_id().alias('songplay_id'),\n",
    "                                      log_df_filter.start_time,\n",
    "                                      log_df_filter.userId,\n",
    "                                      log_df_filter.level,\n",
    "                                      song_df.song_id,\n",
    "                                      song_df.artist_id,\n",
    "                                      log_df_filter.sessionId,\n",
    "                                      log_df_filter.location,\n",
    "                                      log_df_filter.userAgent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data in songplay_df\n",
    "songplay_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save songplay table to s3 in parquet format\n",
    "songplay_df.write.partitionBy(\"year\", \"month\").parquet(\"{}/songplay_table.parquet\".format(output_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
